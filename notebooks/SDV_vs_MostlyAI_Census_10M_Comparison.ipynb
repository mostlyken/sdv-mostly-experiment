{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd96a6c1-930d-4988-bd26-0fe16026fe44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WJZteD10A1eR"
   },
   "source": [
    "# Synthetic Data Generation: SDV vs MOSTLY AI Comparison\n",
    "\n",
    "## Framework Comparison\n",
    "This notebook compares two synthetic data generation approaches on a large-scale dataset:\n",
    "\n",
    "- **SDV (Synthetic Data Vault)** - Business Source License\n",
    "- **MOSTLY AI SDK** - Apache 2.0 License - Open Source\n",
    "\n",
    "## Dataset & Objective\n",
    "We'll use the **US Census Income dataset (10M records)** to:\n",
    "- Compare training performance and generation speed\n",
    "- Evaluate synthetic data quality using comprehensive metrics\n",
    "- Assess privacy preservation capabilities\n",
    "- Provide practical guidance for framework selection\n",
    "\n",
    "## Key Takeaways\n",
    "- Performance benchmarks on large-scale data\n",
    "- Quality comparison metrics\n",
    "- Privacy assessment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93cbceeb-4217-475a-9e33-970940389ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%uv pip install -U sdv mostlyai-qa 'mostlyai[local]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d34158a-9d83-4fcc-9e76-910be946db5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8FAyiL_6BCAb"
   },
   "source": [
    "# 1. Data Preparation\n",
    "\n",
    "## Loading the Dataset\n",
    "We'll use the US Census Income dataset with 10M records containing demographic, employment, and financial information - ideal for testing synthetic data generation at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a4fab2-b3da-4895-8d1a-f9c345862ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the US Census dataset (10M records) from remote Parquet file\n",
    "# Note: This is a large dataset - initial load may take a few minutes\n",
    "data = pd.read_parquet('https://mostly-public-tutorials.s3.eu-central-1.amazonaws.com/datasets/census/census_10_mil.parquet')\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Age | Workclass         | FNLWGT | Education  | Education Num | Marital Status      | Occupation         | Relationship   | Race  | Sex    | Capital Gain | Capital Loss | Hours/Week | Native Country  | Income |\n",
    "|-----|-------------------|--------|------------|----------------|----------------------|---------------------|----------------|-------|--------|---------------|---------------|-------------|------------------|--------|\n",
    "| 39  | State-gov         | 77516  | Bachelors  | 13             | Never-married        | Adm-clerical        | Not-in-family  | White | Male   | 2174          | 0             | 40          | United-States     | <=50K  |\n",
    "| 50  | Self-emp-not-inc  | 83311  | Bachelors  | 13             | Married-civ-spouse   | Exec-managerial     | Husband        | White | Male   | 0             | 0             | 13          | United-States     | <=50K  |\n",
    "| 38  | Private           | 215646 | HS-grad    | 9              | Divorced             | Handlers-cleaners   | Not-in-family  | White | Male   | 0             | 0             | 40          | United-States     | <=50K  |\n",
    "| 53  | Private           | 234721 | 11th       | 7              | Married-civ-spouse   | Handlers-cleaners   | Husband        | Black | Male   | 0             | 0             | 40          | United-States     | <=50K  |\n",
    "| 28  | Private           | 338409 | Bachelors  | 13             | Married-civ-spouse   | Prof-specialty      | Wife           | Black | Female | 0             | 0             | 40          | Cuba              | <=50K  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0e0f02-bc1d-4fc3-b484-b96d2a5962a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DpCW_u-D3won"
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset contains 15 columns with mixed data types:\n",
    "- **Numerical**: age, fnlwgt, education_num, capital_gain, capital_loss, hours_per_week\n",
    "- **Categorical**: workclass, education, marital_status, occupation, relationship, race, sex, native_country, income\n",
    "\n",
    "This combination of numerical and categorical data makes it ideal for testing both frameworks' capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059a9dd0-694f-4967-994c-596e9911065a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUlVgIhZBdrF",
    "outputId": "89e00361-bb0a-4966-ecf3-65e9765ab620"
   },
   "outputs": [],
   "source": [
    "# Display column names and basic data types\n",
    "print(\"Column names:\")\n",
    "print(data.columns.tolist())\n",
    "print(f\"\\nData types:\")\n",
    "print(data.dtypes)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Holdout Split\n",
    "\n",
    "We split the data into:\n",
    "- **Training Set (80% - 8M records)**: For model training\n",
    "- **Holdout Set (20% - 2M records)**: For quality evaluation\n",
    "\n",
    "This split ensures we can properly assess synthetic data quality against unseen real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training/holdout sets\n",
    "# Using stratified split would be better for classification tasks, but not critical here\n",
    "# random_state=1 ensures reproducible results\n",
    "train, holdout = train_test_split(\n",
    "    data, \n",
    "    test_size=0.2,      # 20% for holdout evaluation\n",
    "    random_state=1,     # Fixed seed for reproducibility\n",
    "    shuffle=True        # Ensure random sampling\n",
    ")\n",
    "\n",
    "print(f\"Training set: {train.shape[0]:,} records ({train.shape[0]/len(data)*100:.1f}%)\")\n",
    "print(f\"Holdout set:  {holdout.shape[0]:,} records ({holdout.shape[0]/len(data)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split maintains similar distributions\n",
    "print(f\"\\nIncome distribution in training set:\")\n",
    "print(train['income'].value_counts(normalize=True))\n",
    "print(f\"\\nIncome distribution in holdout set:\")\n",
    "print(holdout['income'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398f0aff-00d9-4773-a344-ee5d6f310104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "v63wmyVvBmtE"
   },
   "source": [
    "# 2. SDV Metadata Configuration\n",
    "\n",
    "## Metadata Setup\n",
    "SDV requires metadata to understand your data structure. We'll use auto-detection to identify column types (numerical vs categorical), then validate the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f3bf12-edc2-4b32-85b5-4a53ab96121b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "95rccasYBo0V"
   },
   "source": [
    "## Auto-Detecting Metadata\n",
    "SDV can automatically detect column types from the data. The auto-detection correctly identifies our numerical and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab60592a-58d9-49d4-b3e9-d9eff4c959a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sdv.metadata import Metadata\n",
    "\n",
    "# Auto-detect metadata from the training data\n",
    "# Note: We wrap the DataFrame in a dict with table name 'table' as required by SDV\n",
    "# Using only training data to avoid data leakage\n",
    "metadata = Metadata.detect_from_dataframes({'table': train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0804b372-97e9-4f30-8c2c-855ad5d31f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the auto-detected metadata\n",
    "print('Auto-detected metadata structure:\\n')\n",
    "print(metadata)\n",
    "\n",
    "# Show a summary of detected column types\n",
    "table_metadata = metadata.to_dict()['tables']['table']['columns']\n",
    "numerical_cols = [col for col, info in table_metadata.items() if info['sdtype'] == 'numerical']\n",
    "categorical_cols = [col for col, info in table_metadata.items() if info['sdtype'] == 'categorical']\n",
    "\n",
    "print(f\"\\nðŸ“Š Metadata Summary:\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83262398-5026-4991-8040-2d3833fb0e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate the metadata structure\n",
    "try:\n",
    "    metadata.validate()\n",
    "    print(\"âœ… Metadata validation passed\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Metadata validation failed: {e}\")\n",
    "    # You would fix metadata issues here if any exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53575b5-2d05-4b02-a194-5937059e9ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate that the metadata matches the actual data structure\n",
    "try:\n",
    "    metadata.validate_data(data=({'table': train}))  # Use train data for consistency\n",
    "    print(\"âœ… Data validation against metadata passed\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Data validation failed: {e}\")\n",
    "    # This would indicate mismatches between metadata and actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec289d7-4a7e-4f48-bf9c-2846f30aca5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "N9S6Kz0RCvph"
   },
   "source": [
    "# 3. SDV: Training and Generation\n",
    "\n",
    "## Gaussian Copula Synthesizer\n",
    "We'll use SDV's Gaussian Copula Synthesizer, which models the statistical relationships between variables and generates synthetic data that preserves these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2779372e-d3c0-498f-af5a-ecb262a34831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "\n",
    "# Initialize the synthesizer with our metadata\n",
    "# GaussianCopula is good for mixed data types and preserving correlations\n",
    "synthesizer = GaussianCopulaSynthesizer(metadata)\n",
    "\n",
    "print(\"ðŸš€ Starting SDV training...\")\n",
    "print(f\"Training on {len(train):,} records with {len(train.columns)} features\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the synthesizer on our training data\n",
    "# This learns the statistical relationships between variables\n",
    "synthesizer.fit(train)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"âœ… SDV training completed in {elapsed_minutes:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ² Starting SDV synthetic data generation...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate synthetic data with the same number of rows as original dataset\n",
    "# You can adjust num_rows based on your needs\n",
    "target_rows = len(data)  # Generate same size as original\n",
    "sdv_synthetic_data = synthesizer.sample(num_rows=target_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"âœ… SDV generation completed in {elapsed_minutes:.2f} minutes\")\n",
    "print(f\"â±ï¸  Generation rate: {target_rows / (end_time - start_time):,.0f} records/second\")\n",
    "print(f\"ðŸ“Š Generated {len(sdv_synthetic_data):,} synthetic records\")\n",
    "\n",
    "# Quick preview of generated data\n",
    "print(\"\\nFirst 5 synthetic records:\")\n",
    "print(sdv_synthetic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443b80fc-a7ff-45fd-bc4b-88b782cc8693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save SDV synthetic data\n",
    "output_file = './data/sdv_synthetic_data.parquet'\n",
    "sdv_synthetic_data.to_parquet(output_file, index=False)\n",
    "\n",
    "# Get file size in MB\n",
    "file_size_mb = os.path.getsize(output_file) / 1024**2\n",
    "\n",
    "print(f\"ðŸ’¾ SDV synthetic data saved to: {output_file}\")\n",
    "print(f\"ðŸ“ File size: {file_size_mb:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mostly AI: Training and Generation\n",
    "\n",
    "## Deep Learning Approach\n",
    "Mostly AI uses advanced deep learning models optimized for tabular data. The SDK provides local training capabilities with configurable parameters for training time and privacy settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74465b4f-99f4-4f55-9e6d-0837ef583c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mostlyai.sdk import MostlyAI\n",
    "\n",
    "# Initialize Mostly AI SDK for local training\n",
    "# local=True means we'll train models locally rather than using cloud API\n",
    "print(\"ðŸ”§ Initializing Mostly AI SDK...\")\n",
    "mostly = MostlyAI(local=True)\n",
    "print(\"âœ… Mostly AI SDK initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850d199c-0ed6-4c9f-a6d2-c96d37c86cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Starting Mostly AI training...\")\n",
    "print(f\"Training on {len(train):,} records with {len(train.columns)} features\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configure and start training\n",
    "# Mostly AI automatically detects column types and optimizes model architecture\n",
    "g = mostly.train(\n",
    "    config={\n",
    "        \"name\": \"US Census Income 10 million\",\n",
    "        \"tables\": [\n",
    "            {\n",
    "                \"name\": \"census\",\n",
    "                \"data\": train,\n",
    "                \"tabularModelConfiguration\": {\n",
    "                    \"max_training_time\": 100,  # Limit training time (minutes)\n",
    "                    # Optional: Add differential privacy\n",
    "                    # 'differential_privacy': {\n",
    "                    #     'max_epsilon': 5.0,      # Privacy budget\n",
    "                    #     'delta': 1e-5,           # Privacy parameter\n",
    "                    # }\n",
    "                    # Optional: Model tuning\n",
    "                    # \"max_epochs\": 50,\n",
    "                    # \"batch_size\": 1024,\n",
    "                },\n",
    "                # Optional: Column-specific configurations\n",
    "                # \"columns\": {\n",
    "                #     \"income\": {\"encode\": \"target\"},  # Mark as target variable\n",
    "                # }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    start=True,  # Start training immediately\n",
    "    wait=True,   # Wait for completion before proceeding\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"âœ… Mostly AI training completed in {elapsed_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ² Starting Mostly AI synthetic data generation...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate synthetic data using the trained generator\n",
    "# size parameter controls how many records to generate\n",
    "target_rows = len(data)\n",
    "sd = mostly.generate(g, size=target_rows)\n",
    "mostlyai_synthetic_data = sd.data()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"âœ… Mostly AI generation completed in {elapsed_minutes:.2f} minutes\")\n",
    "print(f\"â±ï¸  Generation rate: {target_rows / (end_time - start_time):,.0f} records/second\")\n",
    "print(f\"ðŸ“Š Generated {len(mostlyai_synthetic_data):,} synthetic records\")\n",
    "\n",
    "# Quick preview of generated data\n",
    "print(\"\\nFirst 5 synthetic records:\")\n",
    "print(mostlyai_synthetic_data.head())\n",
    "\n",
    "# Basic quality check\n",
    "print(f\"\\nMissing values in synthetic data: {mostlyai_synthetic_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Mostly AI synthetic data for comparison\n",
    "output_file = './data/mostlyai_synthetic_data.parquet'\n",
    "mostlyai_synthetic_data.to_parquet(output_file, index=False)\n",
    "file_size_bytes = os.path.getsize(output_file)\n",
    "print(f\"ðŸ’¾ Mostly AI synthetic data saved to: {output_file}\")\n",
    "print(f\"ðŸ“ File size: {file_size_bytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Quality Assessment and Comparison\n",
    "\n",
    "## Evaluation Framework\n",
    "We'll use Mostly AI's comprehensive QA framework to evaluate both synthetic datasets. The assessment includes:\n",
    "\n",
    "- **Accuracy Metrics**: How well synthetic data preserves statistical distributions (univariate, bivariate, trivariate)\n",
    "- **Similarity Analysis**: Comparison between training, holdout, and synthetic data\n",
    "- **DCR Privacy Metrics**: Distance to Closest Record analysis for privacy assessment\n",
    "- **Overall Quality Score**: Combined metric for synthetic data fidelity\n",
    "\n",
    "### Key Privacy Metrics:\n",
    "- **DCR Share**: Proportion of synthetic records that are closer to holdout than training data (higher = better privacy)\n",
    "- **DCR Training**: Average distance from synthetic to closest training record (higher = better privacy)\n",
    "- **Optimal DCR Share**: ~0.5 indicates good balance between utility and privacy\n",
    "\n",
    "Let's compare the results from both frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the quality assessment framework\n",
    "from mostlyai import qa\n",
    "\n",
    "# Initialize logging to see detailed evaluation progress\n",
    "qa.init_logging()\n",
    "print(\"ðŸ” Quality assessment framework initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d638406c-f4e5-4e80-917d-79bc88305ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Evaluating SDV synthetic data quality...\")\n",
    "\n",
    "# Load the SDV synthetic dataset\n",
    "sdv_synthetic_data = pd.read_parquet('./data/sdv_synthetic_data.parquet')\n",
    "\n",
    "# Run comprehensive quality assessment\n",
    "# This compares synthetic data against training and holdout sets\n",
    "report_path, metrics = qa.report(\n",
    "    syn_tgt_data=sdv_synthetic_data,    # SDV synthetic data\n",
    "    trn_tgt_data=train,                 # Original training data\n",
    "    hol_tgt_data=holdout,               # Holdout data for validation\n",
    "    max_sample_size_embeddings=10_000,  # Limit sample size for efficiency\n",
    "    report_path='sdv_qa_report.html'    # HTML report output\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“‹ SDV Quality Report saved to: {report_path}\")\n",
    "print(\"\\nðŸ“ˆ SDV Quality Metrics:\")\n",
    "print(metrics.model_dump_json(indent=4))\n",
    "\n",
    "# Extract key metrics for comparison\n",
    "sdv_accuracy = metrics.accuracy.overall\n",
    "sdv_dcr_share = metrics.distances.dcr_share\n",
    "sdv_dcr_training = metrics.distances.dcr_training\n",
    "print(f\"\\nðŸŽ¯ SDV Summary:\")\n",
    "print(f\"   Overall Accuracy: {sdv_accuracy:.3f}\")\n",
    "print(f\"   DCR Share: {sdv_dcr_share:.3f} (higher is better for privacy)\")\n",
    "print(f\"   DCR Training: {sdv_dcr_training:.3f} (higher is better for privacy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = train.select_dtypes(include='number').columns\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "train_scaled = train.copy()\n",
    "train_scaled[numeric_cols] = scaler.fit_transform(train[numeric_cols])\n",
    "\n",
    "holdout_scaled = holdout.copy()\n",
    "holdout_scaled[numeric_cols] = scaler.transform(holdout[numeric_cols])\n",
    "\n",
    "mostlyai_scaled = mostlyai_synthetic_data.copy()\n",
    "mostlyai_scaled[numeric_cols] = scaler.transform(mostlyai_synthetic_data[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Evaluating Mostly AI synthetic data quality...\")\n",
    "\n",
    "# Load the Mostly AI synthetic dataset\n",
    "mostlyai_synthetic_data = pd.read_parquet('./data/mostlyai_synthetic_data.parquet')\n",
    "\n",
    "# Run comprehensive quality assessment for Mostly AI\n",
    "report_path, metrics = qa.report(\n",
    "    syn_tgt_data=mostlyai_synthetic_data,  # Mostly AI synthetic data\n",
    "    trn_tgt_data=train,                    # Original training data\n",
    "    hol_tgt_data=holdout,                  # Holdout data for validation\n",
    "    max_sample_size_embeddings=10_000,     # Limit sample size for efficiency\n",
    "    report_path='mostlyai_qa_report.html'  # HTML report output\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“‹ Mostly AI Quality Report saved to: {report_path}\")\n",
    "print(\"\\nðŸ“ˆ Mostly AI Quality Metrics:\")\n",
    "print(metrics.model_dump_json(indent=4))\n",
    "\n",
    "# Extract key metrics for comparison\n",
    "mai_accuracy = metrics.accuracy.overall\n",
    "mai_dcr_share = metrics.distances.dcr_share\n",
    "mai_dcr_training = metrics.distances.dcr_training\n",
    "print(f\"\\nðŸŽ¯ Mostly AI Summary:\")\n",
    "print(f\"   Overall Accuracy: {mai_accuracy:.3f}\")\n",
    "print(f\"   DCR Share: {mai_dcr_share:.3f} (higher is better for privacy)\")\n",
    "print(f\"   DCR Training: {mai_dcr_training:.3f} (higher is better for privacy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ† FINAL COMPARISON\n",
      "============================================================\n",
      "SDV      - Accuracy: 0.738, DCR Share: 0.538\n",
      "MostlyAI - Accuracy: 0.979, DCR Share: 0.507\n",
      "\n",
      "Interpretation:\n",
      "â€¢ Higher accuracy = better statistical fidelity\n",
      "â€¢ Higher DCR Share = better privacy preservation (more diverse synthetic records)\n",
      "â€¢ DCR Share ~0.5 indicates good balance between utility and privacy\n",
      "â€¢ Check HTML reports for detailed analysis\n"
     ]
    }
   ],
   "source": [
    "# Add a final comparison section\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ† FINAL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SDV      - Accuracy: {sdv_accuracy:.3f}, DCR Share: {sdv_dcr_share:.3f}\")\n",
    "print(f\"MostlyAI - Accuracy: {mai_accuracy:.3f}, DCR Share: {mai_dcr_share:.3f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"â€¢ Higher accuracy = better statistical fidelity\")\n",
    "print(\"â€¢ Higher DCR Share = better privacy preservation (more diverse synthetic records)\")\n",
    "print(\"â€¢ DCR Share ~0.5 indicates good balance between utility and privacy\")\n",
    "print(\"â€¢ Check HTML reports for detailed analysis\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Julio - Adult 10 million - SDV",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
