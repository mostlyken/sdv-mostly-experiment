{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd96a6c1-930d-4988-bd26-0fe16026fe44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WJZteD10A1eR"
   },
   "source": [
    "# Synthetic Data Generation: SDV vs MOSTLY AI Comparison\n",
    "\n",
    "## Framework Comparison\n",
    "This notebook compares two synthetic data generation approaches on a large-scale dataset:\n",
    "\n",
    "- **SDV (Synthetic Data Vault)** - Business Source License\n",
    "- **MOSTLY AI SDK** - Apache 2.0 License - Open Source\n",
    "\n",
    "## Dataset & Objective\n",
    "We'll use the **US Census Income dataset (10M records)** to:\n",
    "- Compare training performance and generation speed\n",
    "- Evaluate synthetic data quality using comprehensive metrics\n",
    "- Assess privacy preservation capabilities\n",
    "- Provide practical guidance for framework selection\n",
    "\n",
    "## Key Takeaways\n",
    "- Performance benchmarks on large-scale data\n",
    "- Quality comparison metrics\n",
    "- Privacy assessment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93cbceeb-4217-475a-9e33-970940389ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%uv pip install -U sdv mostlyai-qa 'mostlyai[local]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d34158a-9d83-4fcc-9e76-910be946db5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8FAyiL_6BCAb"
   },
   "source": [
    "# 1. Data Preparation\n",
    "\n",
    "## Loading the Dataset\n",
    "We'll use the US Census Income dataset with 10M records containing demographic, employment, and financial information - ideal for testing synthetic data generation at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a4fab2-b3da-4895-8d1a-f9c345862ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the US Census dataset (10M records) from remote Parquet file\n",
    "# Note: This is a large dataset - initial load may take a few minutes\n",
    "data = pd.read_parquet('https://mostly-public-tutorials.s3.eu-central-1.amazonaws.com/datasets/census/census_10_mil.parquet')\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Age | Workclass         | FNLWGT | Education  | Education Num | Marital Status      | Occupation         | Relationship   | Race  | Sex    | Capital Gain | Capital Loss | Hours/Week | Native Country  | Income |\n",
    "|-----|-------------------|--------|------------|----------------|----------------------|---------------------|----------------|-------|--------|---------------|---------------|-------------|------------------|--------|\n",
    "| 39  | State-gov         | 77516  | Bachelors  | 13             | Never-married        | Adm-clerical        | Not-in-family  | White | Male   | 2174          | 0             | 40          | United-States     | <=50K  |\n",
    "| 50  | Self-emp-not-inc  | 83311  | Bachelors  | 13             | Married-civ-spouse   | Exec-managerial     | Husband        | White | Male   | 0             | 0             | 13          | United-States     | <=50K  |\n",
    "| 38  | Private           | 215646 | HS-grad    | 9              | Divorced             | Handlers-cleaners   | Not-in-family  | White | Male   | 0             | 0             | 40          | United-States     | <=50K  |\n",
    "| 53  | Private           | 234721 | 11th       | 7              | Married-civ-spouse   | Handlers-cleaners   | Husband        | Black | Male   | 0             | 0             | 40          | United-States     | <=50K  |\n",
    "| 28  | Private           | 338409 | Bachelors  | 13             | Married-civ-spouse   | Prof-specialty      | Wife           | Black | Female | 0             | 0             | 40          | Cuba              | <=50K  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0e0f02-bc1d-4fc3-b484-b96d2a5962a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DpCW_u-D3won"
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset contains 15 columns with mixed data types:\n",
    "- **Numerical**: age, fnlwgt, education_num, capital_gain, capital_loss, hours_per_week\n",
    "- **Categorical**: workclass, education, marital_status, occupation, relationship, race, sex, native_country, income\n",
    "\n",
    "This combination of numerical and categorical data makes it ideal for testing both frameworks' capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059a9dd0-694f-4967-994c-596e9911065a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUlVgIhZBdrF",
    "outputId": "89e00361-bb0a-4966-ecf3-65e9765ab620"
   },
   "outputs": [],
   "source": [
    "# Display column names and basic data types\n",
    "print(\"Column names:\")\n",
    "print(data.columns.tolist())\n",
    "print(f\"\\nData types:\")\n",
    "print(data.dtypes)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Holdout Split\n",
    "\n",
    "We split the data into:\n",
    "- **Training Set (80% - 8M records)**: For model training\n",
    "- **Holdout Set (20% - 2M records)**: For quality evaluation\n",
    "\n",
    "This split ensures we can properly assess synthetic data quality against unseen real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training/holdout sets\n",
    "# Using stratified split would be better for classification tasks, but not critical here\n",
    "# random_state=1 ensures reproducible results\n",
    "train, holdout = train_test_split(\n",
    "    data, \n",
    "    test_size=0.2,      # 20% for holdout evaluation\n",
    "    random_state=1,     # Fixed seed for reproducibility\n",
    "    shuffle=True        # Ensure random sampling\n",
    ")\n",
    "\n",
    "print(f\"Training set: {train.shape[0]:,} records ({train.shape[0]/len(data)*100:.1f}%)\")\n",
    "print(f\"Holdout set:  {holdout.shape[0]:,} records ({holdout.shape[0]/len(data)*100:.1f}%)\")\n",
    "\n",
    "# Verify the split maintains similar distributions\n",
    "print(f\"\\nIncome distribution in training set:\")\n",
    "print(train['income'].value_counts(normalize=True))\n",
    "print(f\"\\nIncome distribution in holdout set:\")\n",
    "print(holdout['income'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398f0aff-00d9-4773-a344-ee5d6f310104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "v63wmyVvBmtE"
   },
   "source": [
    "# 2. SDV Metadata Configuration\n",
    "\n",
    "## Metadata Setup\n",
    "SDV requires metadata to understand your data structure. We'll use auto-detection to identify column types (numerical vs categorical), then validate the configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f3bf12-edc2-4b32-85b5-4a53ab96121b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "95rccasYBo0V"
   },
   "source": [
    "## Auto-Detecting Metadata\n",
    "SDV can automatically detect column types from the data. The auto-detection correctly identifies our numerical and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab60592a-58d9-49d4-b3e9-d9eff4c959a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sdv.metadata import Metadata\n",
    "\n",
    "# Auto-detect metadata from the training data\n",
    "# Note: We wrap the DataFrame in a dict with table name 'table' as required by SDV\n",
    "# Using only training data to avoid data leakage\n",
    "metadata = Metadata.detect_from_dataframes({'table': train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0804b372-97e9-4f30-8c2c-855ad5d31f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the auto-detected metadata\n",
    "print('Auto-detected metadata structure:\\n')\n",
    "print(metadata)\n",
    "\n",
    "# Show a summary of detected column types\n",
    "table_metadata = metadata.to_dict()['tables']['table']['columns']\n",
    "numerical_cols = [col for col, info in table_metadata.items() if info['sdtype'] == 'numerical']\n",
    "categorical_cols = [col for col, info in table_metadata.items() if info['sdtype'] == 'categorical']\n",
    "\n",
    "print(f\"\\n📊 Metadata Summary:\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83262398-5026-4991-8040-2d3833fb0e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate the metadata structure\n",
    "try:\n",
    "    metadata.validate()\n",
    "    print(\"✅ Metadata validation passed\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Metadata validation failed: {e}\")\n",
    "    # You would fix metadata issues here if any exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d53575b5-2d05-4b02-a194-5937059e9ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate that the metadata matches the actual data structure\n",
    "try:\n",
    "    metadata.validate_data(data=({'table': train}))  # Use train data for consistency\n",
    "    print(\"✅ Data validation against metadata passed\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Data validation failed: {e}\")\n",
    "    # This would indicate mismatches between metadata and actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec289d7-4a7e-4f48-bf9c-2846f30aca5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "N9S6Kz0RCvph"
   },
   "source": [
    "# 3. SDV: Training and Generation\n",
    "\n",
    "## Gaussian Copula Synthesizer\n",
    "We'll use SDV's Gaussian Copula Synthesizer, which models the statistical relationships between variables and generates synthetic data that preserves these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2779372e-d3c0-498f-af5a-ecb262a34831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "\n",
    "# Initialize the synthesizer with our metadata\n",
    "# GaussianCopula is good for mixed data types and preserving correlations\n",
    "synthesizer = GaussianCopulaSynthesizer(metadata)\n",
    "\n",
    "print(\"🚀 Starting SDV training...\")\n",
    "print(f\"Training on {len(train):,} records with {len(train.columns)} features\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the synthesizer on our training data\n",
    "# This learns the statistical relationships between variables\n",
    "synthesizer.fit(train)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"✅ SDV training completed in {elapsed_minutes:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 Starting SDV synthetic data generation...\n",
      "✅ SDV generation completed in 1.56 minutes\n",
      "⏱️  Generation rate: 106,792 records/second\n",
      "📊 Generated 10,000,000 synthetic records\n",
      "\n",
      "First 5 synthetic records:\n",
      "   age workclass  fnlwgt     education  education_num      marital_status  \\\n",
      "0   29   Private  201054       HS-grad             11            Divorced   \n",
      "1   24   Private  114657  Some-college             10       Never-married   \n",
      "2   30         ?  106626    Assoc-acdm              9  Married-civ-spouse   \n",
      "3   41         ?  134349  Some-college              8  Married-civ-spouse   \n",
      "4   26   Private  197185     Assoc-voc             14       Never-married   \n",
      "\n",
      "       occupation   relationship   race     sex  capital_gain  capital_loss  \\\n",
      "0  Prof-specialty        Husband  White    Male         85331             0   \n",
      "1           Sales  Not-in-family  White  Female         42103          4356   \n",
      "2    Adm-clerical        Husband  White    Male             0            29   \n",
      "3           Sales        Husband  White    Male         83475             0   \n",
      "4   Other-service  Not-in-family  White  Female          9460             0   \n",
      "\n",
      "   hours_per_week native_country income  \n",
      "0              38    Philippines  <=50K  \n",
      "1              20  United-States  <=50K  \n",
      "2              42  United-States  <=50K  \n",
      "3              34  United-States   >50K  \n",
      "4              35  United-States  <=50K  \n"
     ]
    }
   ],
   "source": [
    "print(\"🎲 Starting SDV synthetic data generation...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate synthetic data with the same number of rows as original dataset\n",
    "# You can adjust num_rows based on your needs\n",
    "target_rows = len(data)  # Generate same size as original\n",
    "sdv_synthetic_data = synthesizer.sample(num_rows=target_rows)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"✅ SDV generation completed in {elapsed_minutes:.2f} minutes\")\n",
    "print(f\"⏱️  Generation rate: {target_rows / (end_time - start_time):,.0f} records/second\")\n",
    "print(f\"📊 Generated {len(sdv_synthetic_data):,} synthetic records\")\n",
    "\n",
    "# Quick preview of generated data\n",
    "print(\"\\nFirst 5 synthetic records:\")\n",
    "print(sdv_synthetic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "443b80fc-a7ff-45fd-bc4b-88b782cc8693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save SDV synthetic data\n",
    "output_file = './data/sdv_synthetic_data.parquet'\n",
    "sdv_synthetic_data.to_parquet(output_file, index=False)\n",
    "\n",
    "# Get file size in MB\n",
    "file_size_mb = os.path.getsize(output_file) / 1024**2\n",
    "\n",
    "print(f\"💾 SDV synthetic data saved to: {output_file}\")\n",
    "print(f\"📁 File size: {file_size_mb:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Mostly AI: Training and Generation\n",
    "\n",
    "## Deep Learning Approach\n",
    "Mostly AI uses advanced deep learning models optimized for tabular data. The SDK provides local training capabilities with configurable parameters for training time and privacy settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74465b4f-99f4-4f55-9e6d-0837ef583c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mostlyai.sdk import MostlyAI\n",
    "\n",
    "# Initialize Mostly AI SDK for local training\n",
    "# local=True means we'll train models locally rather than using cloud API\n",
    "print(\"🔧 Initializing Mostly AI SDK...\")\n",
    "mostly = MostlyAI(local=True)\n",
    "print(\"✅ Mostly AI SDK initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850d199c-0ed6-4c9f-a6d2-c96d37c86cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"🚀 Starting Mostly AI training...\")\n",
    "print(f\"Training on {len(train):,} records with {len(train.columns)} features\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Configure and start training\n",
    "# Mostly AI automatically detects column types and optimizes model architecture\n",
    "g = mostly.train(\n",
    "    config={\n",
    "        \"name\": \"US Census Income 10 million\",\n",
    "        \"tables\": [\n",
    "            {\n",
    "                \"name\": \"census\",\n",
    "                \"data\": train,\n",
    "                \"tabularModelConfiguration\": {\n",
    "                    \"max_training_time\": 100,  # Limit training time (minutes)\n",
    "                    # Optional: Add differential privacy\n",
    "                    # 'differential_privacy': {\n",
    "                    #     'max_epsilon': 5.0,      # Privacy budget\n",
    "                    #     'delta': 1e-5,           # Privacy parameter\n",
    "                    # }\n",
    "                    # Optional: Model tuning\n",
    "                    # \"max_epochs\": 50,\n",
    "                    # \"batch_size\": 1024,\n",
    "                },\n",
    "                # Optional: Column-specific configurations\n",
    "                # \"columns\": {\n",
    "                #     \"income\": {\"encode\": \"target\"},  # Mark as target variable\n",
    "                # }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    start=True,  # Start training immediately\n",
    "    wait=True,   # Wait for completion before proceeding\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"✅ Mostly AI training completed in {elapsed_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎲 Starting Mostly AI synthetic data generation...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate synthetic data using the trained generator\n",
    "# size parameter controls how many records to generate\n",
    "target_rows = len(data)\n",
    "sd = mostly.generate(g, size=target_rows)\n",
    "mostlyai_synthetic_data = sd.data()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_minutes = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"✅ Mostly AI generation completed in {elapsed_minutes:.2f} minutes\")\n",
    "print(f\"⏱️  Generation rate: {target_rows / (end_time - start_time):,.0f} records/second\")\n",
    "print(f\"📊 Generated {len(mostlyai_synthetic_data):,} synthetic records\")\n",
    "\n",
    "# Quick preview of generated data\n",
    "print(\"\\nFirst 5 synthetic records:\")\n",
    "print(mostlyai_synthetic_data.head())\n",
    "\n",
    "# Basic quality check\n",
    "print(f\"\\nMissing values in synthetic data: {mostlyai_synthetic_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Mostly AI synthetic data for comparison\n",
    "output_file = './data/mostlyai_synthetic_data.parquet'\n",
    "mostlyai_synthetic_data.to_parquet(output_file, index=False)\n",
    "file_size_bytes = os.path.getsize(output_file)\n",
    "print(f\"💾 Mostly AI synthetic data saved to: {output_file}\")\n",
    "print(f\"📁 File size: {file_size_bytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Quality Assessment and Comparison\n",
    "\n",
    "## Evaluation Framework\n",
    "We'll use Mostly AI's comprehensive QA framework to evaluate both synthetic datasets. The assessment includes:\n",
    "\n",
    "- **Accuracy Metrics**: How well synthetic data preserves statistical distributions (univariate, bivariate, trivariate)\n",
    "- **Similarity Analysis**: Comparison between training, holdout, and synthetic data\n",
    "- **DCR Privacy Metrics**: Distance to Closest Record analysis for privacy assessment\n",
    "- **Overall Quality Score**: Combined metric for synthetic data fidelity\n",
    "\n",
    "### Key Privacy Metrics:\n",
    "- **DCR Share**: Proportion of synthetic records that are closer to holdout than training data (higher = better privacy)\n",
    "- **DCR Training**: Average distance from synthetic to closest training record (higher = better privacy)\n",
    "- **Optimal DCR Share**: ~0.5 indicates good balance between utility and privacy\n",
    "\n",
    "Let's compare the results from both frameworks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize the quality assessment framework\n",
    "from mostlyai import qa\n",
    "\n",
    "# Initialize logging to see detailed evaluation progress\n",
    "qa.init_logging()\n",
    "print(\"🔍 Quality assessment framework initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d638406c-f4e5-4e80-917d-79bc88305ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"📊 Evaluating SDV synthetic data quality...\")\n",
    "\n",
    "# Load the SDV synthetic dataset\n",
    "sdv_synthetic_data = pd.read_parquet('./data/sdv_synthetic_data.parquet')\n",
    "\n",
    "# Run comprehensive quality assessment\n",
    "# This compares synthetic data against training and holdout sets\n",
    "report_path, metrics = qa.report(\n",
    "    syn_tgt_data=sdv_synthetic_data,    # SDV synthetic data\n",
    "    trn_tgt_data=train,                 # Original training data\n",
    "    hol_tgt_data=holdout,               # Holdout data for validation\n",
    "    max_sample_size_embeddings=10_000,  # Limit sample size for efficiency\n",
    "    report_path='sdv_qa_report.html'    # HTML report output\n",
    ")\n",
    "\n",
    "print(f\"📋 SDV Quality Report saved to: {report_path}\")\n",
    "print(\"\\n📈 SDV Quality Metrics:\")\n",
    "print(metrics.model_dump_json(indent=4))\n",
    "\n",
    "# Extract key metrics for comparison\n",
    "sdv_accuracy = metrics.accuracy.overall\n",
    "sdv_dcr_share = metrics.distances.dcr_share\n",
    "sdv_dcr_training = metrics.distances.dcr_training\n",
    "print(f\"\\n🎯 SDV Summary:\")\n",
    "print(f\"   Overall Accuracy: {sdv_accuracy:.3f}\")\n",
    "print(f\"   DCR Share: {sdv_dcr_share:.3f} (higher is better for privacy)\")\n",
    "print(f\"   DCR Training: {sdv_dcr_training:.3f} (higher is better for privacy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = train.select_dtypes(include='number').columns\n",
    "\n",
    "# Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "train_scaled = train.copy()\n",
    "train_scaled[numeric_cols] = scaler.fit_transform(train[numeric_cols])\n",
    "\n",
    "holdout_scaled = holdout.copy()\n",
    "holdout_scaled[numeric_cols] = scaler.transform(holdout[numeric_cols])\n",
    "\n",
    "mostlyai_scaled = mostlyai_synthetic_data.copy()\n",
    "mostlyai_scaled[numeric_cols] = scaler.transform(mostlyai_synthetic_data[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating Mostly AI synthetic data quality...\n",
      "[2025-07-07 13:34:18,433] INFO   : prepared training data for accuracy: (8000000, 15)\n",
      "[2025-07-07 13:34:22,417] INFO   : prepared holdout data for accuracy: (2000000, 15)\n",
      "[2025-07-07 13:36:10,966] INFO   : prepared synthetic data for accuracy: (10000000, 15)\n",
      "[2025-07-07 13:36:11,559] INFO   : encode datasets for embeddings\n",
      "[2025-07-07 13:36:12,303] INFO   : calculated embeddings: syn=(10000, 25), trn=(10000, 25), hol=(10000, 25)\n",
      "[2025-07-07 13:36:12,304] INFO   : report accuracy and correlations\n",
      "[2025-07-07 13:36:12,304] INFO   : calculate original data bins\n",
      "[2025-07-07 13:36:21,451] INFO   : store original data bins\n",
      "[2025-07-07 13:36:21,467] INFO   : calculate synthetic data bins\n",
      "[2025-07-07 13:36:33,379] INFO   : calculate correlations\n",
      "[2025-07-07 13:36:39,297] INFO   : calculate correlations\n",
      "[2025-07-07 13:36:44,477] INFO   : calculated univariates for 15 columns in 1.35 seconds\n",
      "[2025-07-07 13:36:52,777] INFO   : calculated bivariate accuracies for 210 combinations in 8.30 seconds\n",
      "[2025-07-07 13:37:56,454] INFO   : calculated trivariate accuracies for 455 combinations in 63.68 seconds\n",
      "[2025-07-07 13:38:00,078] INFO   : calculate numeric univariate kdes\n",
      "[2025-07-07 13:38:16,765] INFO   : calculate numeric univariate kdes\n",
      "[2025-07-07 13:38:36,878] INFO   : calculate categorical univariate counts\n",
      "[2025-07-07 13:38:38,963] INFO   : calculate categorical univariate counts\n",
      "[2025-07-07 13:38:52,705] INFO   : calculated univariate bin counts for 15 columns in 1.30 seconds\n",
      "[2025-07-07 13:39:00,103] INFO   : calculated bivariate bin counts for 210 combinations in 7.40 seconds\n",
      "[2025-07-07 13:39:01,110] INFO   : calculated univariate bin counts for 15 columns in 0.99 seconds\n",
      "[2025-07-07 13:39:08,851] INFO   : calculated bivariate bin counts for 210 combinations in 7.74 seconds\n",
      "[2025-07-07 13:39:08,851] INFO   : plot univariates\n",
      "[2025-07-07 13:39:08,985] INFO   : plot bivariates\n",
      "[2025-07-07 13:39:10,847] INFO   : report similarity\n",
      "[2025-07-07 13:39:10,848] INFO   : calculate centroid similarities\n",
      "[2025-07-07 13:39:10,851] INFO   : calculated cosine similarity for trn and hol: 0.9999037\n",
      "[2025-07-07 13:39:10,852] INFO   : calculated cosine similarity for trn and syn: 0.9998296\n",
      "[2025-07-07 13:39:10,852] INFO   : calculate discriminator AUC\n",
      "[2025-07-07 13:39:12,242] INFO   : auc_scores=[0.5044, 0.5079, 0.5121, 0.5143, 0.4963, 0.483, 0.5148, 0.4951, 0.4796, 0.5071]\n",
      "[2025-07-07 13:39:12,243] INFO   : calculated AUC for trn and hol: 50.1% in 1.39 seconds\n",
      "[2025-07-07 13:39:14,119] INFO   : auc_scores=[0.5067, 0.5148, 0.5247, 0.5241, 0.5389, 0.5193, 0.5208, 0.5096, 0.5226, 0.5362]\n",
      "[2025-07-07 13:39:14,119] INFO   : calculated AUC for trn and syn: 52.2% in 1.88 seconds\n",
      "[2025-07-07 13:39:14,120] INFO   : plot and store PCA similarity contours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "overflow encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "overflow encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "overflow encountered in matmul\n",
      "\n",
      "/Users/kennethhamilton/Desktop/sdv-mostly-experiment/venv/lib/python3.10/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in matmul\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 13:39:15,775] INFO   : calculate and plot distances\n",
      "[2025-07-07 13:39:15,775] INFO   : calculate distances\n",
      "[2025-07-07 13:39:15,969] INFO   : calculated DCRs for data.shape=(10000, 25) and query.shape=(10000, 25) in 0.18s\n",
      "[2025-07-07 13:39:16,145] INFO   : calculated DCRs for data.shape=(10000, 25) and query.shape=(10000, 25) in 0.18s\n",
      "[2025-07-07 13:39:16,329] INFO   : calculated DCRs for data.shape=(10000, 25) and query.shape=(10000, 25) in 0.18s\n",
      "[2025-07-07 13:39:16,331] INFO   : DCR Share: 50.1%, NNDR Ratio: 0.379 - ALL columns\n",
      "[2025-07-07 13:39:16,370] INFO   : calculated DCRs for data.shape=(10000, 11) and query.shape=(10000, 11) in 0.04s\n",
      "[2025-07-07 13:39:16,414] INFO   : calculated DCRs for data.shape=(10000, 11) and query.shape=(10000, 11) in 0.04s\n",
      "[2025-07-07 13:39:16,454] INFO   : calculated DCRs for data.shape=(10000, 11) and query.shape=(10000, 11) in 0.04s\n",
      "[2025-07-07 13:39:16,455] INFO   : DCR Share: 50.3%, NNDR Ratio: 0.981 - 11 columns [[0, 3, 4, 5, 10, 11, 14, 15, 18, 19, 24]]\n",
      "[2025-07-07 13:39:16,472] INFO   : calculated DCRs for data.shape=(10000, 6) and query.shape=(10000, 6) in 0.02s\n",
      "[2025-07-07 13:39:16,489] INFO   : calculated DCRs for data.shape=(10000, 6) and query.shape=(10000, 6) in 0.02s\n",
      "[2025-07-07 13:39:16,515] INFO   : calculated DCRs for data.shape=(10000, 6) and query.shape=(10000, 6) in 0.03s\n",
      "[2025-07-07 13:39:16,516] INFO   : DCR Share: 50.2%, NNDR Ratio: 0.865 - 6 columns [[1, 16, 17, 20, 21, 22]]\n",
      "[2025-07-07 13:39:16,533] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.02s\n",
      "[2025-07-07 13:39:16,549] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.02s\n",
      "[2025-07-07 13:39:16,566] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.02s\n",
      "[2025-07-07 13:39:16,567] INFO   : DCR Share: 49.3%, NNDR Ratio: 0.556 - 8 columns [[2, 6, 7, 8, 9, 12, 13, 23]]\n",
      "[2025-07-07 13:39:16,583] INFO   : calculated DCRs for data.shape=(10000, 9) and query.shape=(10000, 9) in 0.02s\n",
      "[2025-07-07 13:39:16,600] INFO   : calculated DCRs for data.shape=(10000, 9) and query.shape=(10000, 9) in 0.02s\n",
      "[2025-07-07 13:39:16,616] INFO   : calculated DCRs for data.shape=(10000, 9) and query.shape=(10000, 9) in 0.02s\n",
      "[2025-07-07 13:39:16,618] INFO   : DCR Share: 49.9%, NNDR Ratio: 0.641 - 9 columns [[21, 17, 2, 9, 7, 15, 8, 6, 24]]\n",
      "[2025-07-07 13:39:16,646] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.03s\n",
      "[2025-07-07 13:39:16,673] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.03s\n",
      "[2025-07-07 13:39:16,702] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.03s\n",
      "[2025-07-07 13:39:16,702] INFO   : DCR Share: 50.5%, NNDR Ratio: 0.806 - 8 columns [[13, 18, 19, 23, 0, 4, 12, 1]]\n",
      "[2025-07-07 13:39:16,716] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.01s\n",
      "[2025-07-07 13:39:16,731] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.01s\n",
      "[2025-07-07 13:39:16,747] INFO   : calculated DCRs for data.shape=(10000, 8) and query.shape=(10000, 8) in 0.02s\n",
      "[2025-07-07 13:39:16,748] INFO   : DCR Share: 50.7%, NNDR Ratio: 0.386 - 8 columns [[20, 10, 16, 5, 3, 14, 22, 11]]\n",
      "[2025-07-07 13:39:16,748] INFO   : DCR Share: 50.7%, NNDR Ratio: 0.386 - FINAL\n",
      "[2025-07-07 13:39:16,748] INFO   : plot and store distances\n",
      "[2025-07-07 13:39:16,766] INFO   : calculate metrics\n",
      "[2025-07-07 13:39:16,836] INFO   : report stored at mostlyai_qa_report.html\n",
      "📋 Mostly AI Quality Report saved to: mostlyai_qa_report.html\n",
      "\n",
      "📈 Mostly AI Quality Metrics:\n",
      "{\n",
      "    \"accuracy\": {\n",
      "        \"overall\": 0.9788206,\n",
      "        \"univariate\": 0.9889167,\n",
      "        \"bivariate\": 0.9800195,\n",
      "        \"trivariate\": 0.9675255,\n",
      "        \"coherence\": null,\n",
      "        \"overall_max\": 0.9989773,\n",
      "        \"univariate_max\": 0.9996607,\n",
      "        \"bivariate_max\": 0.9991341,\n",
      "        \"trivariate_max\": 0.998137,\n",
      "        \"coherence_max\": null\n",
      "    },\n",
      "    \"similarity\": {\n",
      "        \"cosine_similarity_training_synthetic\": 0.9998296,\n",
      "        \"cosine_similarity_training_holdout\": 0.9999037,\n",
      "        \"discriminator_auc_training_synthetic\": 0.522,\n",
      "        \"discriminator_auc_training_holdout\": 0.501\n",
      "    },\n",
      "    \"distances\": {\n",
      "        \"ims_training\": 0.006,\n",
      "        \"ims_holdout\": 0.005,\n",
      "        \"ims_trn_hol\": 0.185,\n",
      "        \"dcr_training\": 0.008,\n",
      "        \"dcr_holdout\": 0.008,\n",
      "        \"dcr_trn_hol\": 0.008,\n",
      "        \"dcr_share\": 0.507,\n",
      "        \"nndr_training\": 0.000737270527,\n",
      "        \"nndr_holdout\": 0.00190820294,\n",
      "        \"nndr_trn_hol\": 6.479e-8\n",
      "    }\n",
      "}\n",
      "\n",
      "🎯 Mostly AI Summary:\n",
      "   Overall Accuracy: 0.979\n",
      "   DCR Share: 0.507 (higher is better for privacy)\n",
      "   DCR Training: 0.008 (higher is better for privacy)\n"
     ]
    }
   ],
   "source": [
    "print(\"📊 Evaluating Mostly AI synthetic data quality...\")\n",
    "\n",
    "# Load the Mostly AI synthetic dataset\n",
    "mostlyai_synthetic_data = pd.read_parquet('./data/mostlyai_synthetic_data.parquet')\n",
    "\n",
    "# Run comprehensive quality assessment for Mostly AI\n",
    "report_path, metrics = qa.report(\n",
    "    syn_tgt_data=mostlyai_synthetic_data,  # Mostly AI synthetic data\n",
    "    trn_tgt_data=train,                    # Original training data\n",
    "    hol_tgt_data=holdout,                  # Holdout data for validation\n",
    "    max_sample_size_embeddings=10_000,     # Limit sample size for efficiency\n",
    "    report_path='mostlyai_qa_report.html'  # HTML report output\n",
    ")\n",
    "\n",
    "print(f\"📋 Mostly AI Quality Report saved to: {report_path}\")\n",
    "print(\"\\n📈 Mostly AI Quality Metrics:\")\n",
    "print(metrics.model_dump_json(indent=4))\n",
    "\n",
    "# Extract key metrics for comparison\n",
    "mai_accuracy = metrics.accuracy.overall\n",
    "mai_dcr_share = metrics.distances.dcr_share\n",
    "mai_dcr_training = metrics.distances.dcr_training\n",
    "print(f\"\\n🎯 Mostly AI Summary:\")\n",
    "print(f\"   Overall Accuracy: {mai_accuracy:.3f}\")\n",
    "print(f\"   DCR Share: {mai_dcr_share:.3f} (higher is better for privacy)\")\n",
    "print(f\"   DCR Training: {mai_dcr_training:.3f} (higher is better for privacy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🏆 FINAL COMPARISON\n",
      "============================================================\n",
      "SDV      - Accuracy: 0.738, DCR Share: 0.538, DCR Training: 0.285\n",
      "MostlyAI - Accuracy: 0.979, DCR Share: 0.507, DCR Training: 0.008\n",
      "\n",
      "Interpretation:\n",
      "• Higher accuracy = better statistical fidelity\n",
      "• Higher DCR Share = better privacy preservation (more diverse synthetic records)\n",
      "• Higher DCR Training = better privacy preservation (synthetic records farther from training data)\n",
      "• DCR Share ~0.5 indicates good balance between utility and privacy\n",
      "• Check HTML reports for detailed analysis\n"
     ]
    }
   ],
   "source": [
    "# Add a final comparison section\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🏆 FINAL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SDV      - Accuracy: {sdv_accuracy:.3f}, DCR Share: {sdv_dcr_share:.3f}, DCR Training: {sdv_dcr_training:.3f}\")\n",
    "print(f\"MostlyAI - Accuracy: {mai_accuracy:.3f}, DCR Share: {mai_dcr_share:.3f}, DCR Training: {mai_dcr_training:.3f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"• Higher accuracy = better statistical fidelity\")\n",
    "print(\"• Higher DCR Share = better privacy preservation (more diverse synthetic records)\")\n",
    "print(\"• Higher DCR Training = better privacy preservation (synthetic records farther from training data)\")\n",
    "print(\"• DCR Share ~0.5 indicates good balance between utility and privacy\")\n",
    "print(\"• Check HTML reports for detailed analysis\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Julio - Adult 10 million - SDV",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
